import logging
import os
from dotenv import load_dotenv
from typing import TypedDict, Dict, Optional, Annotated, Any
from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage
import math, ast, re, json, datetime
import pandas as pd
from bs4 import BeautifulSoup
import requests
from rag.rag_cancer_stats import get_cancer_survival_rate

# ======================
# ë¡œê±° ì„¤ì •
# ======================
logging.basicConfig(level=logging.INFO)
LOGGER = logging.getLogger("app")

load_dotenv()
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
if not OPENAI_API_KEY and os.path.exists("api_key.txt"):
    with open("api_key.txt", "r", encoding="utf-8") as f:
        OPENAI_API_KEY = f.read().strip()
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
if not OPENAI_API_KEY:
    raise RuntimeError("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env ë˜ëŠ” api_key.txtë¥¼ í™•ì¸í•˜ì„¸ìš”.")

# =============================
# 2) ë°ì´í„° ë¡œë“œ (ê³„ì‚°ìš© CSV)
# =============================
BASEHAZ_PATH = "data/Anujin_240828_ACS_baseline.csv"
COEFS_PATH   = "data/coefficients.csv"
try:
    basehazard_acs = pd.read_csv(BASEHAZ_PATH)
    coefs_acs      = pd.read_csv(COEFS_PATH)
except Exception as e:
    raise RuntimeError(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")

# ======================
# 1. ìƒíƒœ ì •ì˜ (TypedDict)
# ======================
class InputState(TypedDict):
    raw_query: str   # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ìì—°ì–´ ì§ˆë¬¸
    query_type: Optional[str]  # ì§ˆë¬¸ ìœ í˜• (ì˜ˆ: 'ìƒì¡´ë¶„ì„', 'ì¼ë°˜ì§ˆë¬¸' ë“±)
    extracted: Dict[str, Any]  # ë³€ìˆ˜ ì¶”ì¶œ ê²°ê³¼

class ModelSelectState(TypedDict):
    raw_query: str   # ë‚˜ì´, ì„±ë³„, stage, ê¸°ê°„ ë“± íŒŒì‹±ëœ êµ¬ì¡°í™” ë°ì´í„°
    model_id: str
    extracted: Dict[str, Any]        # ì„ íƒëœ ëª¨ë¸ ì´ë¦„

class CalcState(TypedDict):
    survival_prob: float
    extracted: Dict[str, Any]

class OutputState(TypedDict):
    answer: str      # ìµœì¢… ì‚¬ìš©ì ì‘ë‹µ ë©”ì‹œì§€


# ======================
# 2. ë…¸ë“œ í•¨ìˆ˜ ì •ì˜
# ======================

classifier = ChatOpenAI(model="gpt-4.1", temperature=0)

def classify_question(state: InputState) -> InputState:
    query = state["raw_query"]

    prompt = f"""
    ë„ˆëŠ” ì…ë ¥ ì§ˆë¬¸ì„ 2ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” ì—­í• ì´ì•¼.
    ë°˜ë“œì‹œ ì•„ë˜ ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•´:
    - survival : ìƒì¡´ë¶„ì„/ìƒì¡´ í™•ë¥  ê³„ì‚° ê´€ë ¨
    - chat     : ì¼ë°˜ ì˜í•™/ì¼ë°˜ ëŒ€í™”

    ì§ˆë¬¸: "{query}"
    ë‹µë³€:
    """
    ans = classifier.invoke(prompt).content.strip().lower()

    if ans not in ["survival", "chat"]:
        ans = "chat"  # fallback

    LOGGER.info(f"[NODE] classify_question | raw_query={query}, query_type={ans}")
    return {**state, "query_type": ans}

## Modelì—ì„œ Variable ì¶”ì¶œ

extraction_llm = ChatOpenAI(model="gpt-4.1", temperature=0)

def variable_extraction(state: InputState) -> InputState:
    """ì‚¬ìš©ì ì…ë ¥(raw_query)ì—ì„œ age, sex, stage, yearë¥¼ ì¶”ì¶œí•´ JSON ë°˜í™˜"""
    query = state["raw_query"]
    prompt = f"""
    User question: "{query}"

    Task: Extract survival fields from the text below.
    Respond strictly in JSON format with keys: age, sex, stage, year.
    
    - age: integer
    - sex: male=0, female=1
    - stage: 1=Localized, 2=Regional, 3=Distant, 4=Unknown
    - year: survival duration in years (not calendar year)
    """

    # LLM í˜¸ì¶œ
    messages = [HumanMessage(content=prompt)]
    response = extraction_llm.invoke(messages)

    # JSON íŒŒì‹± (ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜)
    try:
        parsed = json.loads(response.content)
    except Exception:
        parsed = {"age": None, "sex": None, "stage": None, "year": None}

    # state ì—…ë°ì´íŠ¸
    state["extracted"] = parsed

    LOGGER.info(f"[NODE] variable_extraction | extracted_variablees={parsed}")

    return state

## Retreiver ë¨¼ì € ì •ì˜

MODEL_DOCS = [
    {"name": "model_1", "desc": "Uses only sex variable"},
    {"name": "model_2", "desc": "Uses only age variable"},
    {"name": "model_3", "desc": "Uses only seer stage variable"},
    {"name": "model_4", "desc": "Uses sex and age variables"},
    {"name": "model_5", "desc": "Uses sex and seer stage variables"},
    {"name": "model_6", "desc": "Uses age and seer stage variables"},
    {"name": "model_7", "desc": "Uses sex, age, and seer stage variables"},
]
texts = [f"{d['name']}: {d['desc']}" for d in MODEL_DOCS]

embeddings = OpenAIEmbeddings()
model_db = FAISS.from_texts(texts, embeddings)
model_retriever = model_db.as_retriever()

model_select_llm = ChatOpenAI(model="gpt-4.1", temperature=0)

def select_model(state: InputState) -> ModelSelectState:
    extracted = state.get("extracted", {})
    present_vars = [k for k, v in extracted.items() if v is not None]

    # Retrieverë¡œ í›„ë³´êµ° ê²€ìƒ‰
    enriched_query = f"Variables detected: {', '.join(present_vars)}"
    candidates = model_retriever.invoke(enriched_query)
    top_k = 5  # or 3
    candidates = candidates[:top_k]
    candidate_texts = "\n".join([doc.page_content for doc in candidates])

    # LLMì´ í›„ë³´ ì¤‘ ìµœì  ëª¨ë¸ ì„ íƒ
    prompt = f"""
    Below are available survival models and their descriptions:

    {candidate_texts}

    The user's variables are: {', '.join(present_vars)}

    Task: Select the best-fitting model among the candidates above.
    Output ONLY the model name (e.g. "model_4").
    """

    response = model_select_llm.invoke(prompt).content.strip()

    LOGGER.info(f"[NODE] select_model | extracted={extracted}, result={response}")
    state["model_id"] = response
    return state

# def parse_input(state: ModelSelectState) -> ModelSelectState:
#     query = state["raw_query"]
#     parsed = {"age": 60, "sex": "F", "stage": 2, "horizon": 5}
#     LOGGER.info(f"[NODE] parse_input | query={query} -> parsed={parsed}")
#     return {"parsed_input": parsed, "model_id": ""}


def run_calculation(state: ModelSelectState) -> CalcState:
    """ë‚´ë¶€ ëª¨ë¸ì„ í†µí•œ ìƒì¡´í™•ë¥  ê³„ì‚° í•¨ìˆ˜ (model_7)."""
    model_id = state["model_id"]

    if model_id != "model_7":
        LOGGER.warning(f"[NODE] run_calculation | model_id={model_id} not supported.")
        return {"survival_prob": None}
    
    parsed = state["extracted"]
    age = parsed['age']
    sex = parsed['sex']
    stage = parsed['stage']
    year = parsed['year']
    age_coef    = float(coefs_acs.loc[coefs_acs['name'] == 'age', 'value'].values[0])
    female_coef = float(coefs_acs.loc[coefs_acs['name'] == 'sex_female', 'value'].values[0])

    seer_coef_df = coefs_acs[coefs_acs['type'] == 'seer']
    seer_map = {int(n): float(v) for n, v in zip(seer_coef_df["name"], seer_coef_df["value"]) if str(n).isdigit()}

    lr = age_coef * age + female_coef * sex + seer_map.get(stage, 0.0)

    nyear = int(year) * 365
    row = basehazard_acs.loc[basehazard_acs['time'] == nyear, 'hazard']
    if row.empty:
        nearest_idx = (basehazard_acs['time'] - nyear).abs().idxmin()
        basehazard = float(basehazard_acs.loc[nearest_idx, 'hazard'])
    else:
        basehazard = float(row.values[0])
    prob = math.exp(-1.0 * basehazard * math.exp(lr))
    LOGGER.info(f"[NODE] run_calculation | model_id={model_id}, parsed={parsed}, prob={prob}")
    return {"survival_prob": prob}

    
def format_output(state: CalcState) -> OutputState:
    prob = state.get("survival_prob")
    extracted = state.get("extracted", {})

    if prob is None:
        return {"answer": "í˜„ì¬ ì…ë ¥ëœ ë³€ìˆ˜ ì¡°í•©ì— ë§ëŠ” ìƒì¡´í™•ë¥  ê³„ì‚° ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."}

    # â‘  ë‚´ë¶€ ëª¨ë¸ ê³„ì‚° ê²°ê³¼
    base_msg = f"ì €í¬ ë‚´ë¶€ ëª¨ë¸ì— ì˜í•œ 5ë…„ ìƒì¡´ í™•ë¥ ì€ ì•½ {prob * 100:.1f}% ì…ë‹ˆë‹¤."

    # â‘¡ RAG ê²€ìƒ‰
    sex_value = extracted.get("sex")
    sex_label = "ì—¬ì„±" if sex_value == 1 else "ë‚¨ì„±" if sex_value == 0 else "ì „ì²´"
    cancer_type = "ìœ„ì•”"  # í˜„ì¬ëŠ” ì˜ˆì‹œ, ë‚˜ì¤‘ì— extractedì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŒ
    year_range = "2018â€“2022"

    rag_result = get_cancer_survival_rate(cancer_type, sex_label, year_range)

    # â‘¢ ìµœì¢… ì¶œë ¥
    answer = (
        f"{base_msg}\n\n"
        f"ğŸ“Š [êµ­ê°€ì•”ì •ë³´ì„¼í„° ì°¸ê³ ]\n"
        f"{rag_result}\n\n"
        "â€» ìœ„ í†µê³„ëŠ” ì „ì²´ ìœ„ì•” í™˜ì ì§‘ë‹¨ì˜ í‰ê· ê°’ì´ë©°, "
        "ì‹¤ì œ ê°œì¸ì˜ ì„ìƒ ìƒí™©ì— ë”°ë¼ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

    LOGGER.info(f"[NODE] format_output | answer={answer}")
    return {"answer": answer}

# def parse_input(state: ModelSelectState) -> ModelSelectState:
#     query = state["raw_query"]
#     parsed = {"age": 60, "sex": "F", "stage": 2, "horizon": 5}
#     LOGGER.info(f"[NODE] parse_input | query={query} -> parsed={parsed}")
#     return {"parsed_input": parsed, "model_id": ""}


def run_calculation(state: ModelSelectState) -> CalcState:
    """ë‚´ë¶€ ëª¨ë¸ì„ í†µí•œ ìƒì¡´í™•ë¥  ê³„ì‚° í•¨ìˆ˜ (model_7)."""
    model_id = state["model_id"]

    if model_id != "model_7":
        LOGGER.warning(f"[NODE] run_calculation | model_id={model_id} not supported.")
        return {"survival_prob": None}
    
    parsed = state["extracted"]
    age = parsed['age']
    sex = parsed['sex']
    stage = parsed['stage']
    year = parsed['year']
    age_coef    = float(coefs_acs.loc[coefs_acs['name'] == 'age', 'value'].values[0])
    female_coef = float(coefs_acs.loc[coefs_acs['name'] == 'sex_female', 'value'].values[0])

    seer_coef_df = coefs_acs[coefs_acs['type'] == 'seer']
    seer_map = {int(n): float(v) for n, v in zip(seer_coef_df["name"], seer_coef_df["value"]) if str(n).isdigit()}

    lr = age_coef * age + female_coef * sex + seer_map.get(stage, 0.0)

    nyear = int(year) * 365
    row = basehazard_acs.loc[basehazard_acs['time'] == nyear, 'hazard']
    if row.empty:
        nearest_idx = (basehazard_acs['time'] - nyear).abs().idxmin()
        basehazard = float(basehazard_acs.loc[nearest_idx, 'hazard'])
    else:
        basehazard = float(row.values[0])
    prob = math.exp(-1.0 * basehazard * math.exp(lr))
    LOGGER.info(f"[NODE] run_calculation | model_id={model_id}, parsed={parsed}, prob={prob}")
    return {"survival_prob": prob}

## êµ­ê°€ì•”í†µê³„ 


def fetch_kcca_stat_text() -> str:
    """êµ­ê°€ì•”ì •ë³´ì„¼í„° í˜ì´ì§€ì—ì„œ ì£¼ìš” ë¬¸ë‹¨ì„ ê°€ì ¸ì˜¤ëŠ” ê°„ë‹¨í•œ RAG ì°¸ì¡°ìš© í•¨ìˆ˜."""
    try:
        url = "https://www.cancer.go.kr/lay1/S1T648C650/contents.do"
        resp = requests.get(url, timeout=5)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        paras = soup.select("div.cont_txt p")
        texts = [p.get_text().strip() for p in paras if p.get_text().strip()]

        return " ".join(texts[:3]) if texts else "êµ­ê°€ì•”ì •ë³´ì„¼í„° í˜ì´ì§€ì—ì„œ í†µê³„ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"í†µê³„ ì •ë³´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


def format_output(state: CalcState) -> OutputState:
    prob = state.get("survival_prob")
    extracted = state.get("extracted", {})

    if prob is None:
        return {"answer": "í˜„ì¬ ì…ë ¥ëœ ë³€ìˆ˜ ì¡°í•©ì— ë§ëŠ” ìƒì¡´í™•ë¥  ê³„ì‚° ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."}

    # -------------------------
    # ë‚´ë¶€ ëª¨ë¸ ê²°ê³¼ ë©”ì‹œì§€
    # -------------------------
    base_msg = f"ì €í¬ ë‚´ë¶€ ëª¨ë¸ì— ì˜í•œ 5ë…„ ìƒì¡´ í™•ë¥ ì€ ì•½ {prob * 100:.1f}% ì…ë‹ˆë‹¤."

    # -------------------------
    # RAG ê²€ìƒ‰ ì‹¤í–‰
    # -------------------------
    sex_value = extracted.get("sex")
    sex_label = "ì—¬ì„±" if sex_value == 1 else "ë‚¨ì„±"
    seer_stage = extracted.get("stage")

    # í˜„ì¬ëŠ” ìœ„ì•” ê³ ì • (ì›í•˜ë©´ cancer_type = extracted.get("cancer_type", "ìœ„ì•”")ë¡œ ì¼ë°˜í™” ê°€ëŠ¥)
    cancer_type = "ìœ„ì•”"
    year_range = "2018â€“2022"

    try:
        stat_summary = get_cancer_survival_rate(cancer_type, sex_label, year_range, seer_stage)
    except Exception as e:
        stat_summary = f"êµ­ê°€ì•”ë“±ë¡í†µê³„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    # -------------------------
    # ê²°ê³¼ í†µí•©
    # -------------------------
    answer = (
        f"{base_msg}\n\n"
        f"[êµ­ê°€ì•”ë“±ë¡í†µê³„ ì°¸ê³ ]\n"
        f"{stat_summary.strip()}\n\n"
        "â€» ìœ„ í†µê³„ëŠ” ì „ì²´ ìœ„ì•” í™˜ì ì§‘ë‹¨ì˜ í‰ê· ê°’ì´ë©°, ì‹¤ì œ ê°œì¸ì˜ ì„ìƒ ìƒí™©ì— ë”°ë¼ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

    LOGGER.info(f"[NODE] format_output | answer={answer}")
    return {"answer": answer}

llm = ChatOpenAI(model="gpt-4.1", temperature=0.7)

def general_chat(state: InputState) -> OutputState:
    query = state["raw_query"]
    answer = llm.invoke(query).content
    LOGGER.info(f"[NODE] general_chat | answer={answer}")
    return {"answer": answer}


# ======================
# 3. ê·¸ë˜í”„ êµ¬ì„±
# ======================d
builder = StateGraph(OutputState, input_schema=InputState, output_schema=OutputState)

# ë…¸ë“œ ì¶”ê°€
builder.add_node("classify_question", classify_question)
builder.add_node("variable_extraction", variable_extraction)
builder.add_node("select_model", select_model)
builder.add_node("run_calculation", run_calculation)
builder.add_node("format_output", format_output)
builder.add_node("general_chat", general_chat)  # ì¼ë°˜ ëŒ€í™” ë…¸ë“œ ì¶”ê°€

# ì‹œì‘ì  â†’ ë¶„ë¥˜ê¸°ë¡œ ì´ë™
builder.add_edge(START, "classify_question")

# classify_question â†’ ë¶„ê¸°
builder.add_conditional_edges(
    "classify_question",
    lambda state: state["query_type"],  # query_type ê°’ìœ¼ë¡œ ë¶„ê¸°
    {
        "survival": "variable_extraction",   # ìƒì¡´ë¶„ì„ â†’ parse_inputìœ¼ë¡œ
        "chat": "general_chat",      # ì¼ë°˜ ëŒ€í™” â†’ general_chatìœ¼ë¡œ
    },
)

# survival pipeline
builder.add_edge("variable_extraction","select_model")
builder.add_edge("select_model", "run_calculation")
builder.add_edge("run_calculation", "format_output")
builder.add_edge("format_output", END)

# chat pipeline
builder.add_edge("general_chat", END)

graph = builder.compile()


if __name__ == "__main__":
    result = graph.invoke({"raw_query": "60ì„¸ ì—¬ì„±, seer stage 2, 5ë…„ ìƒì¡´ í™•ë¥ ??"})
    print(result)
    print(graph.get_graph().draw_mermaid())
