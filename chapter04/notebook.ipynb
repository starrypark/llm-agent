{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LangChain 기초\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. LangChain 개요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain 설치\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-core==0.3.0 langchain-openai==0.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
     "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
     "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
     "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. LLM / Chat model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:34.493540Z",
     "iopub.status.busy": "2024-06-28T02:32:34.493370Z",
     "iopub.status.idle": "2024-06-28T02:32:36.949739Z",
     "shell.execute_reply": "2024-06-28T02:32:36.949284Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "model = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0)\n",
    "ai_message = model.invoke(\"안녕하세요\")\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:36.951724Z",
     "iopub.status.busy": "2024-06-28T02:32:36.951536Z",
     "iopub.status.idle": "2024-06-28T02:32:39.034259Z",
     "shell.execute_reply": "2024-06-28T02:32:39.033764Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant.\"),\n",
    "    HumanMessage(\"안녕하세요! 저는 존이라고 합니다!\"),\n",
    "    AIMessage(content=\"안녕하세요, 존님! 어떤 도움이 필요하신가요?\"),\n",
    "    HumanMessage(content=\"제 이름을 아시나요?\"),\n",
    "]\n",
    "\n",
    "ai_message = model.invoke(messages)\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스트리밍\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:39.036893Z",
     "iopub.status.busy": "2024-06-28T02:32:39.036468Z",
     "iopub.status.idle": "2024-06-28T02:32:40.188998Z",
     "shell.execute_reply": "2024-06-28T02:32:40.188511Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant.\"),\n",
    "    HumanMessage(\"안녕하세요!\"),\n",
    "]\n",
    "\n",
    "for chunk in model.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Prompt template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:40.191537Z",
     "iopub.status.busy": "2024-06-28T02:32:40.191222Z",
     "iopub.status.idle": "2024-06-28T02:32:40.197096Z",
     "shell.execute_reply": "2024-06-28T02:32:40.196704Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"다음 요리의 레시피를 생각해 주세요.\n",
    "\n",
    "요리명: {dish}\"\"\")\n",
    "\n",
    "prompt_value = prompt.invoke({\"dish\": \"카레\"})\n",
    "print(prompt_value.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ＜보충: 프롬프트 변수가 1개인 경우＞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_value = prompt.invoke(\"카레\")\n",
    "print(prompt_value.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:40.199009Z",
     "iopub.status.busy": "2024-06-28T02:32:40.198811Z",
     "iopub.status.idle": "2024-06-28T02:32:40.204792Z",
     "shell.execute_reply": "2024-06-28T02:32:40.204434Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_value = prompt.invoke({\"dish\": \"카레\"})\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MessagesPlaceholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:40.206823Z",
     "iopub.status.busy": "2024-06-28T02:32:40.206639Z",
     "iopub.status.idle": "2024-06-28T02:32:40.213139Z",
     "shell.execute_reply": "2024-06-28T02:32:40.212764Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_value = prompt.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"안녕하세요! 저는 존이라고 합니다!\"),\n",
    "            AIMessage(\"안녕하세요, 존님! 어떻게 도와드릴까요?\"),\n",
    "        ],\n",
    "        \"input\": \"제 이름을 아시나요?\",\n",
    "    }\n",
    ")\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith의 Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:40.215357Z",
     "iopub.status.busy": "2024-06-28T02:32:40.215034Z",
     "iopub.status.idle": "2024-06-28T02:32:40.850572Z",
     "shell.execute_reply": "2024-06-28T02:32:40.850086Z"
    }
   },
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"oshima/recipe\")\n",
    "\n",
    "prompt_value = prompt.invoke({\"dish\": \"카레\"})\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (칼럼) 멀티모달 모델의 입력 처리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:40.852551Z",
     "iopub.status.busy": "2024-06-28T02:32:40.852328Z",
     "iopub.status.idle": "2024-06-28T02:32:40.984619Z",
     "shell.execute_reply": "2024-06-28T02:32:40.984231Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"user\",\n",
    "            [\n",
    "                {\"type\": \"text\", \"text\": \"이미지를 설명해 주세요.\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"{image_url}\"}},\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "image_url = \"https://raw.githubusercontent.com/ychoi-kr/langchain-book/main/assets/cover.jpg\"\n",
    "\n",
    "prompt_value = prompt.invoke({\"image_url\": image_url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:40.986542Z",
     "iopub.status.busy": "2024-06-28T02:32:40.986385Z",
     "iopub.status.idle": "2024-06-28T02:32:49.421870Z",
     "shell.execute_reply": "2024-06-28T02:32:49.421368Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "ai_message = model.invoke(prompt_value)\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Output parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PydanticOutputParser를 사용한 Python 객체 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:49.423970Z",
     "iopub.status.busy": "2024-06-28T02:32:49.423805Z",
     "iopub.status.idle": "2024-06-28T02:32:49.427124Z",
     "shell.execute_reply": "2024-06-28T02:32:49.426747Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
    "    steps: list[str] = Field(description=\"steps to make the dish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:49.428896Z",
     "iopub.status.busy": "2024-06-28T02:32:49.428630Z",
     "iopub.status.idle": "2024-06-28T02:32:49.430921Z",
     "shell.execute_reply": "2024-06-28T02:32:49.430585Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:49.432752Z",
     "iopub.status.busy": "2024-06-28T02:32:49.432482Z",
     "iopub.status.idle": "2024-06-28T02:32:49.435573Z",
     "shell.execute_reply": "2024-06-28T02:32:49.435232Z"
    }
   },
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:49.437304Z",
     "iopub.status.busy": "2024-06-28T02:32:49.437089Z",
     "iopub.status.idle": "2024-06-28T02:32:49.440212Z",
     "shell.execute_reply": "2024-06-28T02:32:49.439868Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"사용자가 입력한 요리의 레시피를 생각해 주세요.\\n\\n\"\n",
    "            \"{format_instructions}\",\n",
    "        ),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_with_format_instructions = prompt.partial(\n",
    "    format_instructions=format_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:49.441965Z",
     "iopub.status.busy": "2024-06-28T02:32:49.441750Z",
     "iopub.status.idle": "2024-06-28T02:32:49.446830Z",
     "shell.execute_reply": "2024-06-28T02:32:49.446490Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_value = prompt_with_format_instructions.invoke({\"dish\": \"카레\"})\n",
    "print(\"=== role: system ===\")\n",
    "print(prompt_value.messages[0].content)\n",
    "print(\"=== role: user ===\")\n",
    "print(prompt_value.messages[1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:49.448688Z",
     "iopub.status.busy": "2024-06-28T02:32:49.448466Z",
     "iopub.status.idle": "2024-06-28T02:32:53.559871Z",
     "shell.execute_reply": "2024-06-28T02:32:53.559376Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "ai_message = model.invoke(prompt_value)\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:53.561786Z",
     "iopub.status.busy": "2024-06-28T02:32:53.561632Z",
     "iopub.status.idle": "2024-06-28T02:32:53.572140Z",
     "shell.execute_reply": "2024-06-28T02:32:53.571665Z"
    }
   },
   "outputs": [],
   "source": [
    "recipe = output_parser.invoke(ai_message)\n",
    "print(type(recipe))\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:53.614251Z",
     "iopub.status.busy": "2024-06-28T02:32:53.614030Z",
     "iopub.status.idle": "2024-06-28T02:32:53.619857Z",
     "shell.execute_reply": "2024-06-28T02:32:53.619506Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "ai_message = AIMessage(content=\"안녕하세요. 저는 AI 어시스턴트입니다.\")\n",
    "ai_message = output_parser.invoke(ai_message)\n",
    "print(type(ai_message))\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Chain—LangChain Expression Language(LCEL) 개요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt와 model 연결\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:53.621888Z",
     "iopub.status.busy": "2024-06-28T02:32:53.621584Z",
     "iopub.status.idle": "2024-06-28T02:32:53.674625Z",
     "shell.execute_reply": "2024-06-28T02:32:53.674143Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:53.676366Z",
     "iopub.status.busy": "2024-06-28T02:32:53.676220Z",
     "iopub.status.idle": "2024-06-28T02:32:53.678436Z",
     "shell.execute_reply": "2024-06-28T02:32:53.678095Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:53.680152Z",
     "iopub.status.busy": "2024-06-28T02:32:53.679899Z",
     "iopub.status.idle": "2024-06-28T02:32:59.546328Z",
     "shell.execute_reply": "2024-06-28T02:32:59.544073Z"
    }
   },
   "outputs": [],
   "source": [
    "ai_message = chain.invoke({\"dish\": \"카레\"})\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StrOutputParser를 연결에 추가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:59.555286Z",
     "iopub.status.busy": "2024-06-28T02:32:59.554550Z",
     "iopub.status.idle": "2024-06-28T02:33:05.105219Z",
     "shell.execute_reply": "2024-06-28T02:33:05.104783Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "output = chain.invoke({\"dish\": \"카레\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PydanticOutputParser를 사용한 연결\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:05.107198Z",
     "iopub.status.busy": "2024-06-28T02:33:05.107009Z",
     "iopub.status.idle": "2024-06-28T02:33:05.110329Z",
     "shell.execute_reply": "2024-06-28T02:33:05.109967Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
    "    steps: list[str] = Field(description=\"steps to make the dish\")\n",
    "\n",
    "\n",
    "output_parser = PydanticOutputParser(pydantic_object=Recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:05.112162Z",
     "iopub.status.busy": "2024-06-28T02:33:05.111890Z",
     "iopub.status.idle": "2024-06-28T02:33:05.167716Z",
     "shell.execute_reply": "2024-06-28T02:33:05.167216Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\\n\\n{format_instructions}\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt_with_format_instructions = prompt.partial(\n",
    "    format_instructions=output_parser.get_format_instructions()\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).bind(\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:05.169740Z",
     "iopub.status.busy": "2024-06-28T02:33:05.169590Z",
     "iopub.status.idle": "2024-06-28T02:33:05.172044Z",
     "shell.execute_reply": "2024-06-28T02:33:05.171669Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = prompt_with_format_instructions | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:05.173987Z",
     "iopub.status.busy": "2024-06-28T02:33:05.173667Z",
     "iopub.status.idle": "2024-06-28T02:33:10.259404Z",
     "shell.execute_reply": "2024-06-28T02:33:10.258892Z"
    }
   },
   "outputs": [],
   "source": [
    "recipe = chain.invoke({\"dish\": \"카레\"})\n",
    "print(type(recipe))\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （칼럼）with_structured_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:10.261391Z",
     "iopub.status.busy": "2024-06-28T02:33:10.261230Z",
     "iopub.status.idle": "2024-06-28T02:33:12.288341Z",
     "shell.execute_reply": "2024-06-28T02:33:12.287844Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    ingredients: list[str] = Field(description=\"ingredients of the dish\")\n",
    "    steps: list[str] = Field(description=\"steps to make the dish\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chain = prompt | model.with_structured_output(Recipe)\n",
    "\n",
    "recipe = chain.invoke({\"dish\": \"카레\"})\n",
    "print(type(recipe))\n",
    "print(recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6.LangChain의 RAG 관련 컴포넌트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community==0.3.0 GitPython==3.1.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./langchain\",\n",
    "    branch=\"master\",\n",
    "    file_filter=file_filter,\n",
    ")\n",
    "\n",
    "raw_docs = loader.load()\n",
    "print(len(raw_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-text-splitters==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "docs = text_splitter.split_documents(raw_docs)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"AWS의 S3에서 데이터를 읽어 들이기 위한 Document loader가 있나요?\"\n",
    "\n",
    "vector = embeddings.embed_query(query)\n",
    "print(len(vector))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-chroma==0.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"AWS의 S3에서 데이터를 읽어 들이기 위한 Document loader가 있나요?\"\n",
    "\n",
    "context_docs = retriever.invoke(query)\n",
    "print(f\"len = {len(context_docs)}\")\n",
    "\n",
    "first_doc = context_docs[0]\n",
    "print(f\"metadata = {first_doc.metadata}\")\n",
    "print(first_doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL을 사용한 RAG Chain 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "다음 문맥만을 바탕으로 질문에 답변해 주세요.\n",
    "\n",
    "문맥: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "질문: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = chain.invoke(query)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
