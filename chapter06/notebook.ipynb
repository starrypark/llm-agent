{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0PKm9G89pnX"
      },
      "source": [
        "# 6. Advanced RAG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
          "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
          "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
          "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
        },
        "id": "h1DZbRDf9pnZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi0JvPpj9pna"
      },
      "source": [
        "## 6.2. 실습 준비\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySknYh5r9pna"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-core==0.3.0 langchain-openai==0.2.0 \\\n",
        "    langchain-community==0.3.0 GitPython==3.1.43 \\\n",
        "    langchain-chroma==0.1.4 tavily-python==0.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRd4uyHB9pnb"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import GitLoader\n",
        "\n",
        "\n",
        "def file_filter(file_path: str) -> bool:\n",
        "    return file_path.endswith(\".mdx\")\n",
        "\n",
        "\n",
        "loader = GitLoader(\n",
        "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
        "    repo_path=\"./langchain\",\n",
        "    branch=\"master\",\n",
        "    file_filter=file_filter,\n",
        ")\n",
        "\n",
        "documents = loader.load()\n",
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZvTJ3cw9pnb"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "db = Chroma.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvOEl5vt9pnb"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template('''\\\n",
        "다음 문맥만을 고려해 질문에 답변해 주세요.\n",
        "\n",
        "문맥: \"\"\"\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "질문: {question}\n",
        "''')\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"context\": retriever,\n",
        "} | prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke(\"LangChain의 개요를 알려줘\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZIE8le_9pnc"
      },
      "source": [
        "## 6.3. 검색 쿼리의 기법\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "netbJFf29pnc"
      },
      "source": [
        "### HyDE（Hypothetical Document Embeddings）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBuabrTW9pnc"
      },
      "outputs": [],
      "source": [
        "hypothetical_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
        "다음 질문에 한 문장으로 답변해 주세요.\n",
        "\n",
        "질문: {question}\n",
        "\"\"\")\n",
        "\n",
        "hypothetical_chain = hypothetical_prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frf6wQ519pnc"
      },
      "outputs": [],
      "source": [
        "hyde_rag_chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"context\": hypothetical_chain | retriever,\n",
        "} | prompt | model | StrOutputParser()\n",
        "\n",
        "hyde_rag_chain.invoke(\"LangChain의 개요를 알려줘\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TP-WZ629pnd"
      },
      "source": [
        "### 복수 검색 쿼리 생성\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSY3kSNv9pnd"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class QueryGenerationOutput(BaseModel):\n",
        "    queries: list[str] = Field(..., description=\"검색 쿼리 목록\")\n",
        "\n",
        "\n",
        "query_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
        "질문에 대해 벡터 데이터베이스에서 관련 문서를 검색하기 위한\n",
        "3개의 서로 다른 검색 쿼리를 생성해 주세요.\n",
        "거리 기반 유사성 검색의 한계를 극복하기 위해\n",
        "사용자의 질문에 대해 여러 관점을 제공하는 것이 목표입니다.\n",
        "\n",
        "질문: {question}\n",
        "\"\"\")\n",
        "\n",
        "query_generation_chain = (\n",
        "    query_generation_prompt\n",
        "    | model.with_structured_output(QueryGenerationOutput)\n",
        "    | (lambda x: x.queries)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12EKtrQ69pnd"
      },
      "outputs": [],
      "source": [
        "multi_query_rag_chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"context\": query_generation_chain | retriever.map(),\n",
        "} | prompt | model | StrOutputParser()\n",
        "\n",
        "multi_query_rag_chain.invoke(\"LangChain의 개요를 알려줘\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1sQkOqa9pnd"
      },
      "source": [
        "## 6.4. 검색 후 기법\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9PGm-sj9pnd"
      },
      "source": [
        "### RAG Fusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umNm-WNj9pnd"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "def reciprocal_rank_fusion(\n",
        "    retriever_outputs: list[list[Document]],\n",
        "    k: int = 60,\n",
        ") -> list[str]:\n",
        "    # 각 문서의 콘텐츠(문자열)와 그 점수의 매핑을 저장하는 딕셔너리 준비\n",
        "    content_score_mapping = {}\n",
        "\n",
        "    # 검색 쿼리마다 반복\n",
        "    for docs in retriever_outputs:\n",
        "        # 검색 결과의 문서마다 반복\n",
        "        for rank, doc in enumerate(docs):\n",
        "            content = doc.page_content\n",
        "\n",
        "            # 처음 등장한 콘텐츠인 경우 점수를 0으로 초기화\n",
        "            if content not in content_score_mapping:\n",
        "                content_score_mapping[content] = 0\n",
        "\n",
        "            # (1 / (순위 + k)) 점수를 추가\n",
        "            content_score_mapping[content] += 1 / (rank + k)\n",
        "\n",
        "    # 점수가 큰 순서로 정렬\n",
        "    ranked = sorted(content_score_mapping.items(), key=lambda x: x[1], reverse=True)  # noqa\n",
        "    return [content for content, _ in ranked]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEUf8gaZ9pnd"
      },
      "outputs": [],
      "source": [
        "rag_fusion_chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"context\": query_generation_chain | retriever.map() | reciprocal_rank_fusion,\n",
        "} | prompt | model | StrOutputParser()\n",
        "\n",
        "rag_fusion_chain.invoke(\"LangChain의 개요를 알려줘\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHim_yAy9pne"
      },
      "source": [
        "### Cohere 리랭크 모델 사용 준비\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HS1R3B8b9pne"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = userdata.get(\"COHERE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqoulfYB9pne"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-cohere==0.3.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acu-x81b9pne"
      },
      "source": [
        "### Cohere 리랭크 모델 도입\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_LFGl8B9pne"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "def rerank(inp: dict[str, Any], top_n: int = 3) -> list[Document]:\n",
        "    question = inp[\"question\"]\n",
        "    documents = inp[\"documents\"]\n",
        "\n",
        "    cohere_reranker = CohereRerank(model=\"rerank-multilingual-v3.0\", top_n=top_n)\n",
        "    return cohere_reranker.compress_documents(documents=documents, query=question)\n",
        "\n",
        "\n",
        "rerank_rag_chain = (\n",
        "    {\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        \"documents\": retriever,\n",
        "    }\n",
        "    | RunnablePassthrough.assign(context=rerank)\n",
        "    | prompt | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "rerank_rag_chain.invoke(\"LangChain의 개요를 알려줘\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAr098mw9pne"
      },
      "source": [
        "## 6.5. 복수 Retriever 활용 기법\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEVBeTuU9pne"
      },
      "source": [
        "### LLM에 의한 라우팅\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sq-2gZa9pne"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
        "\n",
        "langchain_document_retriever = retriever.with_config(\n",
        "    {\"run_name\": \"langchain_document_retriever\"}\n",
        ")\n",
        "\n",
        "web_retriever = TavilySearchAPIRetriever(k=3).with_config(\n",
        "    {\"run_name\": \"web_retriever\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZesCZW8r9pne"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "\n",
        "class Route(str, Enum):\n",
        "    langchain_document = \"langchain_document\"\n",
        "    web = \"web\"\n",
        "\n",
        "\n",
        "class RouteOutput(BaseModel):\n",
        "    route: Route\n",
        "\n",
        "\n",
        "route_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
        "질문에 답변하기 위한 적절한 Retriever를 선택해주세요.\n",
        "\n",
        "질문: {question}\n",
        "\"\"\")\n",
        "\n",
        "route_chain = (\n",
        "    route_prompt\n",
        "    | model.with_structured_output(RouteOutput)\n",
        "    | (lambda x: x.route)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXiF2Qyg9pne"
      },
      "outputs": [],
      "source": [
        "def routed_retriever(inp: dict[str, Any]) -> list[Document]:\n",
        "    question = inp[\"question\"]\n",
        "    route = inp[\"route\"]\n",
        "\n",
        "    if route == Route.langchain_document:\n",
        "        return langchain_document_retriever.invoke(question)\n",
        "    elif route == Route.web:\n",
        "        return web_retriever.invoke(question)\n",
        "\n",
        "    raise ValueError(f\"Unknown route: {route}\")\n",
        "\n",
        "\n",
        "route_rag_chain = (\n",
        "    {\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        \"route\": route_chain,\n",
        "    }\n",
        "    | RunnablePassthrough.assign(context=routed_retriever)\n",
        "    | prompt | model | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKMqpcVH9pnf"
      },
      "outputs": [],
      "source": [
        "route_rag_chain.invoke(\"LangChain의 개요를 알려줘\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1EMocFi9pnf"
      },
      "outputs": [],
      "source": [
        "route_rag_chain.invoke(\"오늘 서울 날씨는?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1haiYKG9pnf"
      },
      "source": [
        "### 하이브리드 검색 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIdr6kAq9pnf"
      },
      "outputs": [],
      "source": [
        "!pip install rank-bm25==0.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mgy04p5q9pnf"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "chroma_retriever = retriever.with_config(\n",
        "    {\"run_name\": \"chroma_retriever\"}\n",
        ")\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents).with_config(\n",
        "    {\"run_name\": \"bm25_retriever\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldZ7g_749pnf"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "hybrid_retriever = (\n",
        "    RunnableParallel({\n",
        "        \"chroma_documents\": chroma_retriever,\n",
        "        \"bm25_documents\": bm25_retriever,\n",
        "    })\n",
        "    | (lambda x: [x[\"chroma_documents\"], x[\"bm25_documents\"]])\n",
        "    | reciprocal_rank_fusion\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrv58PH79pnf"
      },
      "outputs": [],
      "source": [
        "hybrid_rag_chain = (\n",
        "    {\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        \"context\": hybrid_retriever,\n",
        "    }\n",
        "    | prompt | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "hybrid_rag_chain.invoke(\"LangChain의 개요를 알려줘\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4jausDn9pnf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}